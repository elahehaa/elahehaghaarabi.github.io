---
title: 'NLP Must-Reads'
date: 2024-02-19
permalink: /posts/2024/02/n/
tags:
  - Natural Language Processing
  - Transfornmer-based Language Models
  - Contextualized Embedding
  - Computational Linguistics
  - Language Understanding
  - Generative AI
---

Here is an archive of major publications in the field of NLP, providing a historical overview of the discipline.

# Publications

<!-- 
# W2Vec
The term "word2vec" was introduced in the paper titled "Distributed Representations of Words and Phrases and their Compositionality," written by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeffrey Dean. This paper was presented at the Neural Information Processing Systems (NeurIPS) conference in 2013.
W2Vec is a group of shallow neural network models trained to reconstruct linguistic contexts of words. The word2vec models efficiently learn distributed representations (embeddings) of words in a continuous vector space, capturing semantic relationships between words. This paper has had a significant impact on natural language processing and has become one of the foundational works in the field of word embeddings.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546). In Advances in neural information processing systems (NeurIPS), 26.
-->
