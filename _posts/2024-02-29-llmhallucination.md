---
title: 'LLM Hallucination in Healthcare'
date: 2024-05-01
permalink: /posts/2024/02/h/
tags:
  - Transfornmer-based Language Models
  - Generative AI
---



# Semantic Search and Question Answering System

Language models often produce irrelevant, non-factual, and synthetic answers when responding to user queries. This issue, known as hallucination in the field of GenAI, becomes particularly critical when applied in the healthcare domain. To mitigate this problem, we employ the Retrieval Augmented Generation (RAG) technique to guide the model in generating relevant and accurate responses.
======
-->

<!-- You can have many headings
======
-->

<!-- Aren't headings cool?
------ -->
