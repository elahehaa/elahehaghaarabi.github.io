---
title: 'Decoding Methods for Generative AI'
date: 2024-07-18
permalink: /posts/2024/05/b/
tags:
  - Generative AI
  - Large Language Models
  - Decoding techniques
  - Beam Search Algorithm
  - Lookback Lens
  - 
 
---
Here are different techniques for a generative language model to choose the next token or sequence of tokens. I will explain them briefly based on my understanding of new research papers published in the field.

## 1. Greedy
The model selects the token with the highest probability at each step. While computationally efficient, it often leads to repetitive and less diverse outputs.

Example:
Given the following probability distribution for the next token:
- Token A: 0.6
- Token B: 0.3
- Token C: 0.1

Greedy sampling would select Token A because it has the highest probability, resulting in a predictable sequence.

## 2. Sampling
### 2.1. Top-k Sampling
This method restricts sampling to the top-k tokens with the highest probabilities, where k is a predetermined value. It introduces more diversity compared to greedy sampling while still maintaining some control over the generated sequence.

Example:
Given the same probability distribution as above and setting k=2, top-k sampling would consider only Token A and Token B. The model randomly selects one of these tokens based on their probabilities, resulting in a more varied output.

### 2.2. Top-p Sampling (Nucleus Sampling)
Also known as nucleus sampling, this technique involves selecting tokens from the smallest set whose cumulative probability exceeds a certain threshold, p. It dynamically adjusts the set of candidate tokens based on their probabilities.

Example:
Consider a probability distribution with many tokens, where the cumulative probability of the top tokens exceeds a threshold of 0.8. Nucleus sampling would select tokens from this subset, ensuring both diversity and coherence in the generated text.

### 2.3. Temperature Scaling
Temperature scaling modifies the logits produced by the model before applying the softmax function. By dividing logits by a temperature parameter, the distribution of probabilities is adjusted. A temperature < 1 emphasizes high-probability tokens, while a temperature > 1 encourages diversity by smoothing the distribution.

Example:
Given the same probability distribution as above, applying temperature scaling with a value of 0.5 would amplify the differences between probabilities. As a result, Token A, with its higher probability, would be even more likely to be chosen, leading to a more deterministic output.

Beam search is an algorithm utilized in generative models to find the most likely sequence of tokens. It explores multiple potential output sequences simultaneously, aiming to identify the most promising candidate.

## 3. Beam Search

1. **Initialization**: The model initializes a beam of k ( k = 3 candidate sequences.
   - Beam 1: "The cat" _ (possible tokens: sat, ran, slept)
   - Beam 2: "The cat" _ (possible tokens: jumped, purred, played)
   - Beam 3: "The cat" _ (possible tokens: ate, drank, meowed)
   
3. **Expansion**: At each step, the model generates all possible next tokens for each of the k candidate sequences in the beam.
   - For each beam:
     - Beam 1: "The cat sat", "The cat ran", "The cat slept"
     - Beam 2: "The cat jumped", "The cat purred", "The cat played"
     - Beam 3: "The cat ate", "The cat drank", "The cat meowed"
   
5. **Scoring**: The model scores each candidate sequence based on a predefined scoring function.
   
6. **Iteration**: The process continues iteratively until the maximum sequence length is reached.



Through this iterative process, beam search helps the model generate coherent and contextually appropriate sequences of tokens.

## 4. Lookback Lens
This technique is proposed by researchers from MIT and the University of Washington in this [paper](https://arxiv.org/pdf/2407.07071#page=10&zoom=100,88,274).
The authors introduce a novel approach for detecting and mitigating contextual hallucinations in language models using a Logistic Regression classifier. This classifier operates in two stages:

Detection: The classifier first identifies whether the generated sequence of tokens contains any hallucinations. It does this by analyzing the attention maps generated by the language model. Attention maps record how the model processes given contextual information, making them a valuable feature for detecting hallucinations.

Guided Decoding: Once hallucinations are detected, the classifier then guides the language model's decoding process. It helps the model to choose tokens that are less likely to cause hallucinations. By influencing the token selection process, the classifier ensures the generated text remains more factual and consistent with the provided context.

The classifier is trained using attention maps as features, which enables it to generalize well across different language models. One of the key advantages of this method is its transferability. The classifier can be applied to any language model, regardless of its size, without requiring extensive retraining. This makes it a versatile tool for improving the reliability of generated text in various applications.
![lookbacklens.png](https://github.com/elahehaghaarabi/elahehaghaarabi.github.io/blob/master/images/lookbacklens.png)
